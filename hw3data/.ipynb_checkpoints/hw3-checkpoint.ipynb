{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### QUESTION 1 ##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropogation\n",
    "\n",
    "<img src=\"q1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### QUESTION 2 ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a small CNN for MNIST digit classification\n",
    "\n",
    "In this problem, you will train a small convolutional neural network for image classification, using PyTorch. We will use the MNIST dataset for digit classification (http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import struct\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PARSING\n",
    "# If this is running on GPU cluster in the default workspace, no change required\n",
    "# If running on GPU cluster in created directory, change path as required\n",
    "# If running on PC download MNIST dataset from http://yann.lecun.com/exdb/mnist/ and set path\n",
    "path = \"../../../../../MNIST/\"\n",
    "\n",
    "def read(dataset = \"training\", datatype='images'):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "    \n",
    "    if(datatype=='images'):\n",
    "        get_data = lambda idx: img[idx]\n",
    "    elif(datatype=='labels'):\n",
    "        get_data = lambda idx: lbl[idx]\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_data(i)\n",
    "        \n",
    "trainData=np.array(list(read('training','images')))\n",
    "trainData=np.float32(np.expand_dims(trainData,-1))/255\n",
    "torchTrainData=trainData.transpose((0,3,1,2))\n",
    "trainLabels=np.int32(np.array(list(read('training','labels'))))\n",
    "\n",
    "testData=np.array(list(read('testing','images')))\n",
    "testData=np.float32(np.expand_dims(testData,-1))/255\n",
    "torchTestData=testData.transpose((0,3,1,2))\n",
    "testLabels=np.int32(np.array(list(read('testing','labels'))))\n",
    "\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    ind=np.where(trainLabels==i)[0][0]\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(trainData[ind][:,:,0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 3 points ] Define the network structure as follows**\n",
    "\n",
    "* Convolutional layer with 32 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Convolutional layer with 64 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Fully connected layer with 1024 output channels\n",
    "* In place ReLU activation layer\n",
    "* Dropout layer with drop rate 0.4\n",
    "* Fully connected layer with 10 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,drop):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # DEFINE THE NETWORK STRUCTURE\n",
    "        \n",
    "        # Example: self.conv1 = nn.Conv2d(1, 3, 5,stride=1,padding=2,bias=True)\n",
    "        # You can look at https://github.com/ameykusurkar/pytorch-image-classifier/blob/master/main.py for reference\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Example: x = self.conv1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Print net\n",
    "net = Net(drop=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 5 points ] Complete the train function below. Use the same parameters to perform training in each of the following setups:**\n",
    "* SGD for optimization, without dropout\n",
    "* SGD for optimization, with dropout\n",
    "* Adam for optimization, without dropout\n",
    "* Adam for optimization, with dropout.\n",
    "\n",
    "As evaluation for each case above, perform the following:\n",
    "* Plot the loss graph and the accuracy graph on training set on the same plot\n",
    "* Print the accuracy on test set\n",
    "\n",
    "Test accuracies are expected to be quite high (~98 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BELOW IS AN EXAMPLE STARTER\n",
    "# FEEL FREE TO EDIT ANYTHING\n",
    "\n",
    "# to_train is a parameter that determines what part of the net to train\n",
    "# it is not required for this question, but will be useful in the next one\n",
    "def train(tdata,tlabel,net,to_train,opt):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "    \n",
    "    # Change parameters as required\n",
    "    epochs=15\n",
    "    batch=200\n",
    "    learning_rate=1e-3\n",
    "    \n",
    "    if(opt=='adam'):\n",
    "        optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.SGD(to_train,lr=learning_rate,momentum = 0.99)\n",
    "        \n",
    "    for k in tqdm(range(epochs)):\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "            inds=np.random.randint(0,len(tdata)-1,batch)\n",
    "            inputs = Variable(torch.FloatTensor(tdata[inds]).cuda())\n",
    "            targets = Variable(torch.LongTensor(tlabel[inds]).cuda())\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # Train the model using the optimizer and the batch data\n",
    "            # Append the loss and accuracy to the losslist and acclist arrays\n",
    "\n",
    "    return losslist,acclist\n",
    "\n",
    "def test(tdata,tlabel,net):\n",
    "    inputs = Variable(torch.FloatTensor(tdata).cuda())\n",
    "    targets = Variable(torch.LongTensor(tlabel).cuda())\n",
    "    prediction = net(inputs)\n",
    "    acc=np.mean(np.argmax(prediction.data.cpu().numpy(),1)==tlabel)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code \n",
    "\n",
    "net = Net(drop=False).cuda()\n",
    "loss,acc=train(torchTrainData,trainLabels,net,net.parameters(),'sgd')\n",
    "ax=range(len(x1))\n",
    "plt.plot(ax,loss,ax,acc)\n",
    "plt.legend(['loss','accuracy'])\n",
    "plt.show()\n",
    "print('Accuracy:{}'.format(test(torchTestData,testLabels,net)))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 5 points ] Plot the following graphs and comment on them**\n",
    "\n",
    "* Training loss graphs of SGD−dropout and Adam−dropout on the same plot \n",
    "* Training loss graphs for Adam-dropout for 3 different values of batch size such that there is some difference in the graphs, on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 2 points ] Plot the training loss graphs for changes made in any particular parameter (learning rate/momentum etc) such that there is a clear difference in the graphs, on the same plot, and comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  QUESTION 3  ##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transfer learning\n",
    "\n",
    "You will now visualize the effects of transfer learning by performing experiments using the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) . Note that this is just to understand how transfer learning works, in practice it is generally used with very large datasets and complex networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PARSING\n",
    "# If this is running on GPU cluster in the default workspace, no change required\n",
    "# If running on GPU cluster in created directory, change path as required\n",
    "# If running on PC download CIFAR-10 dataset from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and set path\n",
    "path='../../../../../CIFAR-10/cifar-10-batches-py/'\n",
    "data=np.zeros((0,32,32,3))\n",
    "labels=[]\n",
    "for i in range(1,6):\n",
    "    with open(path+'data_batch_'+str(i), 'rb') as fo:\n",
    "        dat = pickle.load(fo,encoding='latin1')\n",
    "        r=dat['data'][:,:1024*1].reshape((10000,32,32,1))\n",
    "        g=dat['data'][:,1024:2048].reshape((10000,32,32,1))\n",
    "        b=dat['data'][:,2048:3072].reshape((10000,32,32,1))\n",
    "        rgb=np.concatenate((r,g,b),axis=3)\n",
    "        data=np.vstack((data,np.float32(rgb)/255))\n",
    "        labels+=dat['labels']\n",
    "labels=np.array(labels)\n",
    "# data -> 50000 X 32 X 32 X 3 array with training data\n",
    "# labels -> 50000 labels ranging from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **[ 2 points ] Plot 3 random images corresponding to each label from the training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and labels into 2 sets, first one containing labels 0 to 4, and second one from 5 to 9. \n",
    "\n",
    "data1=np.zeros((0,32,32,3))\n",
    "labels1=[]\n",
    "data2=np.zeros((0,32,32,3))\n",
    "labels2=[]\n",
    "for i in range(5):\n",
    "    x=data[labels==i]\n",
    "    data1=np.vstack((data1,x))\n",
    "    labels1+=[i]*len(x)\n",
    "for i in range(5,10):\n",
    "    x=data[labels==i]\n",
    "    data2=np.vstack((data2,x))\n",
    "    labels2+=[i-5]*len(x)\n",
    "    \n",
    "labels1=np.array(labels1)\n",
    "labels2=np.array(labels2)\n",
    "\n",
    "torch_data1=data1.transpose((0,3,1,2))\n",
    "torch_data2=data2.transpose((0,3,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 3 points ] Create a simple convolutional network to classify the training data. The network structure should be as follows:**\n",
    "1. Layer 1 - Kernel size 4, Stride 2, Output channels 5, Bias enabled, Relu activation\n",
    "2. Layer 2 - Kernel size 4, Stride 1, Output channels 10, Bias enabled, Relu avtication\n",
    "3. Layer 3 - Kernel size 4, Stride 1, Output channels 20, Bias enabled, Relu activation\n",
    "4. Layer 4 - Kernel size 4, Stride 1, Output channels 40, Bias enabled, Relu activation\n",
    "5. Layer 5 - Fully connected layer with 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 5 points ] Complete the train function below and follow the instructions** \n",
    "\n",
    "* Initialize the network, train the complete network (net.parameters) on data1 (The first 5 classes)\n",
    "* Plot the loss and accuracy graphs over training on the same plot\n",
    "* Print the final training accuracy as well**\n",
    "\n",
    "Set the learning rate, number of iterations and batch size such that the loss is gradually and smoothly decreasing and converging. The accuracy at the end of training must be around or greater than 60 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_train can be net.paramaters OR net.fc.parameters OR net.conv1.parameters so that only certain parts of the net are trained\n",
    "def train(tdata,tlabel,net,to_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "    \n",
    "    # Change as required\n",
    "    epochs=5\n",
    "    batch=100\n",
    "    learning_rate=1e-3\n",
    "    optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    \n",
    "    for k in tqdm(range(epochs)):\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "            inds=np.random.randint(0,len(tdata)-1,batch)\n",
    "            inputs = Variable(torch.FloatTensor(tdata[inds]).cuda())\n",
    "            targets = Variable(torch.LongTensor(tlabel[inds]).cuda())\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return losslist,acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 2 points ] Without reinitializing the network, train only the fully connected layer (net.fc.parameters) now on data2 (The next 5 classes)** \n",
    "\n",
    "Do not change any hyper parameters such as learning rate or batch size. Plot the loss and accuracy and print the final values like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 3 points ] Now repeat the process in the opposite order** \n",
    "\n",
    "* Initialize the net again, train the whole network on data2, generate the same plots as before\n",
    "* Then without reinitializing the net, train only the fully connected layer on data1 and generate the plots\n",
    "\n",
    "Do not change any hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ 5 points ]**\n",
    "\n",
    "* Plot the loss vs iterations for the classifers trained to classify data1, via normal learning as well as transfer learning, on the same plot\n",
    "* Plot another graph for the classifiers trained to classify data2\n",
    "\n",
    "Explain the results obtained, based on the training regimen. Comment on why transfer learning worked/didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a network with more layers, pooling layers, and more filters and try to increase accuracy as much as possible. Play around with the hyperparameters to understand how they affect the training process. No need to turn anything in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
